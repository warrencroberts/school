{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "There are several main adjustments you may try.\n",
    "\n",
    "It is useful to take note of the time it takes the algorithm to train. This will provide you with an additional level of understanding. That is also your first task.\n",
    "\n",
    "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "\n",
    "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "\n",
    "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the method: tf.nn.sigmoid()\n",
    "\n",
    "5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the method: tf.nn.tanh()\n",
    "\n",
    "6. Adjust the batch size. Try a batch size of 1000. How does the required time change? What about the accuracy?\n",
    "\n",
    "7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "\n",
    "8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "\n",
    "10. Combine all the methods above and try to reach a validation accuracy of 98.5+ percent.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" for machine learning because for most students it is their first example. The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional networks. The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. In order to exemplify what we've talked about in this section, we will build a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-917aabef6fe5>:21: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "# This function automatically downloads the MNIST dataset to the chosen directory. \n",
    "# The dataset is already split into training, validation, and test subsets. \n",
    "# Furthermore, it preprocess it into a particularly simple and useful format.\n",
    "# Every 28x28 image is flattened into a vector of length 28x28=784, where every value\n",
    "# corresponds to the intensity of the color of the corresponding pixel.\n",
    "# The samples are grayscale (but standardized from 0 to 1), so a value close to 0 is almost white and a value close to\n",
    "# 1 is almost purely black. This representation (flattening the image row by row into\n",
    "# a vector) is slightly naive but as you'll see it works surprisingly well.\n",
    "# Since this is a classification problem, our targets are categorical.\n",
    "# Recall from the lecture on that topic that one way to deal with that is to use one-hot encoding.\n",
    "# With it, the target for each individual sample is a vector of length 10\n",
    "# which has nine 0s and a single 1 at the position which corresponds to the correct answer.\n",
    "# For instance, if the true answer is \"1\", the target will be [0,0,0,1,0,0,0,0,0,0] (counting from 0).\n",
    "# Have in mind that the very first time you execute this command it might take a little while to run\n",
    "# because it has to download the whole dataset. Following commands only extract it so they're faster.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model\n",
    "\n",
    "The whole code is in one cell, so you can simply rerun this cell (instead of the whole notebook) and train a new model.\n",
    "The tf.reset_default_graph() function takes care of clearing the old parameters. From there on, a completely new training starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_run(width, depth, lr, batch_size):\n",
    "    input_size = 784\n",
    "    output_size = 10\n",
    "    # Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "    hidden_layer_size = width\n",
    "\n",
    "    # Reset any variables left in memory from previous runs.\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # As in the previous example - declare placeholders where the data will be fed into.\n",
    "    inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "    targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "    \n",
    "    weights_input = tf.get_variable(\"weights_input\", [input_size, hidden_layer_size])\n",
    "    biases_input = tf.get_variable(\"biases_input\", [hidden_layer_size])\n",
    "\n",
    "    # Operation between the inputs and the first hidden layer.\n",
    "    # We've chosen ReLu as our activation function. You can try playing with different non-linearities.\n",
    "    outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_input) + biases_input)\n",
    "\n",
    "    new_outputs = outputs_1\n",
    "    for idx in range(depth):\n",
    "#         print(\".... hidden layer\")\n",
    "        layer_inputs = new_outputs\n",
    "        weights = tf.get_variable(\"weights_{0}\".format(idx), [hidden_layer_size, hidden_layer_size])\n",
    "        biases = tf.get_variable(\"biases_{0}\".format(idx), [hidden_layer_size])\n",
    "        new_outputs = tf.nn.relu(tf.matmul(layer_inputs, weights) + biases)\n",
    "\n",
    "    weights_3 = tf.get_variable(\"weights_last\", [hidden_layer_size, output_size])\n",
    "    biases_3 = tf.get_variable(\"biases_last\", [output_size])\n",
    "\n",
    "    outputs = tf.matmul(new_outputs, weights_3) + biases_3\n",
    "\n",
    "    # Calculate the loss function for every output/target pair.\n",
    "    # The function used is the same as applying softmax to the last layer and then calculating cross entropy\n",
    "    # with the function we've seen in the lectures. This function, however, combines them in a clever way, \n",
    "    # which makes it both faster and more numerically stable (when dealing with very small numbers).\n",
    "    # Logits here means: unscaled probabilities (so, the outputs, before they are scaled by the softmax)\n",
    "    # Naturally, the labels are the targets.\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "#    loss = tf.nn.tanh(outputs)\n",
    "    loss = tf.nn.sigmoid(outputs)\n",
    "\n",
    "    # Get the average loss\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Define the optimization step. Using adaptive optimizers such as Adam in TensorFlow\n",
    "    # is as simple as that.\n",
    "    optimize = tf.train.AdamOptimizer(learning_rate=lr).minimize(mean_loss)\n",
    "\n",
    "    # Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "    out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "    # Get the average accuracy of the outputs.\n",
    "    accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "    # Declare the session variable.\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Initialize the variables. Default initializer is Xavier.\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "\n",
    "    # Batching\n",
    "    batch_size = 100\n",
    "\n",
    "    # Calculate the number of batches per epoch for the training set.\n",
    "    batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "    # Basic early stopping. Set a miximum number of epochs.\n",
    "    max_epochs = 15\n",
    "\n",
    "    # Keep track of the validation loss of the previous epoch.\n",
    "    # If the validation loss becomes increasing, we want to trigger early stopping.\n",
    "    # We initially set it at some arbitrarily high number to make sure we don't trigger it\n",
    "    # at the first epoch\n",
    "    prev_validation_loss = 9999999.\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a loop for the epochs. Epoch_counter is a variable which automatically starts from 0.\n",
    "    for epoch_counter in range(max_epochs):\n",
    "        # Keep track of the sum of batch losses in the epoch.\n",
    "        curr_epoch_loss = 0.\n",
    "\n",
    "        # Iterate over the batches in this epoch.\n",
    "        for batch_counter in range(batches_number):\n",
    "\n",
    "            # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "            input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Run the optimization step and get the mean loss for this batch.\n",
    "            # Feed it with the inputs and the targets we just got from the train dataset\n",
    "            _, batch_loss = sess.run([optimize, mean_loss], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            # Increment the sum of batch losses.\n",
    "            curr_epoch_loss += batch_loss\n",
    "\n",
    "        # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "        # We want to find the average batch losses over the whole epoch\n",
    "        # The average batch loss is a good proxy for the current epoch loss\n",
    "        curr_epoch_loss /= batches_number\n",
    "\n",
    "        # At the end of each epoch, get the validation loss and accuracy\n",
    "        # Get the input batch and the target batch from the validation dataset\n",
    "        input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "\n",
    "        # Run without the optimization step (simply forward propagate)\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "        # Print statistics for the current epoch\n",
    "        # Epoch counter + 1, because epoch_counter automatically starts from 0, instead of 1\n",
    "        # We format the losses with 3 digits after the dot\n",
    "        # We format the accuracy in percentages for easier interpretation\n",
    "#         print('Epoch '+str(epoch_counter+1)+\n",
    "#               '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "#               '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "#               '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "\n",
    "        # Trigger early stopping if validation loss begins increasing.\n",
    "        if validation_loss > prev_validation_loss:\n",
    "            break\n",
    "\n",
    "        # Store this epoch's validation loss to be used as previous validation loss in the next iteration.\n",
    "        prev_validation_loss = validation_loss\n",
    "\n",
    "    # Not essential, but it is nice to know when the algorithm stopped working in the output section, rather than check the kernel\n",
    "#     print('End of training.')\n",
    "\n",
    "    #Add the time it took the algorithm to train\n",
    "#     print(\"Training time: %s seconds\" % (time.time() - start_time))\n",
    "    return sess, accuracy, inputs, targets, (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training and validation sets, we test the final prediction power of our model by running it on the test dataset that the algorithm has not seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. The test is the absolute final instance. You should not test before you are completely done with adjusting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "\n",
    "def compute_test_accuracy(sess, train_accuracy, inputs, targets):\n",
    "    \n",
    "    test_accuracy = sess.run([train_accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "    test_accuracy_percent = test_accuracy[0] * 100.\n",
    "    \n",
    "    return test_accuracy_percent\n",
    "\n",
    "\n",
    "    # Print the test accuracy formatted in percentages\n",
    "#     print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly between 97% and 98%. Each time the code is rerunned, we get a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "Finally, we have intentionally reached a suboptimal solution, so you can have space to build on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    ret_str = \"\"\n",
    "    for width in [50, 200, 500]:\n",
    "        for depth in [2, 4, 10]:\n",
    "            for lr in [0.0001, 0.001, 0.01, 0.5]:\n",
    "                batch_size = 1000\n",
    "                print(\"run_test: ({0}, {1}, {2}, {3})\"\\\n",
    "                      .format(width, depth, lr, batch_size))\n",
    "                sess, accuracy, train_inputs, train_targets, run_time = train_and_run(width, depth, lr, batch_size)\n",
    "                test_accuracy = compute_test_accuracy(sess, accuracy, train_inputs, train_targets)\n",
    "                \n",
    "                ret_str += \"{bs},{w},{d},{lr},{acc},{rt}\\n\"\\\n",
    "                    .format(bs=batch_size, w=width, d=depth, lr=lr, acc=test_accuracy, rt=run_time)\n",
    "    return ret_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_test: (50, 2, 0.0001, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_test: (50, 2, 0.001, 1000)\n",
      "run_test: (50, 2, 0.01, 1000)\n",
      "run_test: (50, 2, 0.5, 1000)\n",
      "run_test: (50, 4, 0.0001, 1000)\n",
      "run_test: (50, 4, 0.001, 1000)\n",
      "run_test: (50, 4, 0.01, 1000)\n",
      "run_test: (50, 4, 0.5, 1000)\n",
      "run_test: (50, 10, 0.0001, 1000)\n",
      "run_test: (50, 10, 0.001, 1000)\n",
      "run_test: (50, 10, 0.01, 1000)\n",
      "run_test: (50, 10, 0.5, 1000)\n",
      "run_test: (200, 2, 0.0001, 1000)\n",
      "run_test: (200, 2, 0.001, 1000)\n",
      "run_test: (200, 2, 0.01, 1000)\n",
      "run_test: (200, 2, 0.5, 1000)\n",
      "run_test: (200, 4, 0.0001, 1000)\n",
      "run_test: (200, 4, 0.001, 1000)\n",
      "run_test: (200, 4, 0.01, 1000)\n",
      "run_test: (200, 4, 0.5, 1000)\n",
      "run_test: (200, 10, 0.0001, 1000)\n",
      "run_test: (200, 10, 0.001, 1000)\n",
      "run_test: (200, 10, 0.01, 1000)\n",
      "run_test: (200, 10, 0.5, 1000)\n",
      "run_test: (500, 2, 0.0001, 1000)\n",
      "run_test: (500, 2, 0.001, 1000)\n",
      "run_test: (500, 2, 0.01, 1000)\n",
      "run_test: (500, 2, 0.5, 1000)\n",
      "run_test: (500, 4, 0.0001, 1000)\n",
      "run_test: (500, 4, 0.001, 1000)\n",
      "run_test: (500, 4, 0.01, 1000)\n",
      "run_test: (500, 4, 0.5, 1000)\n",
      "run_test: (500, 10, 0.0001, 1000)\n",
      "run_test: (500, 10, 0.001, 1000)\n",
      "run_test: (500, 10, 0.01, 1000)\n",
      "run_test: (500, 10, 0.5, 1000)\n",
      "1000,50,2,0.0001,11.349999904632568,26.621227502822876\n",
      "1000,50,2,0.001,10.090000182390213,24.669053316116333\n",
      "1000,50,2,0.01,11.349999904632568,26.408268213272095\n",
      "1000,50,2,0.5,11.349999904632568,24.468424081802368\n",
      "1000,50,4,0.0001,11.330000311136246,28.832342386245728\n",
      "1000,50,4,0.001,9.740000218153,29.662790536880493\n",
      "1000,50,4,0.01,11.349999904632568,28.855360507965088\n",
      "1000,50,4,0.5,9.57999974489212,28.43732261657715\n",
      "1000,50,10,0.0001,7.320000231266022,35.13705134391785\n",
      "1000,50,10,0.001,8.919999748468399,35.20077705383301\n",
      "1000,50,10,0.01,8.919999748468399,35.99542713165283\n",
      "1000,50,10,0.5,10.090000182390213,36.12907433509827\n",
      "1000,200,2,0.0001,8.780000358819962,40.036219358444214\n",
      "1000,200,2,0.001,10.429999977350235,41.07687449455261\n",
      "1000,200,2,0.01,8.919999748468399,43.66203999519348\n",
      "1000,200,2,0.5,9.57999974489212,42.3233585357666\n",
      "1000,200,4,0.0001,10.100000351667404,58.01329302787781\n",
      "1000,200,4,0.001,9.82000008225441,14.801393270492554\n",
      "1000,200,4,0.01,9.82000008225441,55.54946851730347\n",
      "1000,200,4,0.5,10.320000350475311,55.08770537376404\n",
      "1000,200,10,0.0001,9.780000150203705,100.55075216293335\n",
      "1000,200,10,0.001,10.100000351667404,100.6274721622467\n",
      "1000,200,10,0.01,11.349999904632568,99.38052797317505\n",
      "1000,200,10,0.5,9.740000218153,102.42563009262085\n",
      "1000,500,2,0.0001,12.510000169277191,80.63400769233704\n",
      "1000,500,2,0.001,10.100000351667404,11.174497127532959\n",
      "1000,500,2,0.01,11.349999904632568,80.7627227306366\n",
      "1000,500,2,0.5,10.279999673366547,110.46242380142212\n",
      "1000,500,4,0.0001,11.840000003576279,3604.077129840851\n",
      "1000,500,4,0.001,10.100000351667404,460.7206220626831\n",
      "1000,500,4,0.01,10.320000350475311,9912.023000240326\n",
      "1000,500,4,0.5,10.279999673366547,780.922789812088\n",
      "1000,500,10,0.0001,8.919999748468399,14400.35309958458\n",
      "1000,500,10,0.001,10.279999673366547,23758.505940914154\n",
      "1000,500,10,0.01,10.100000351667404,235.33800673484802\n",
      "1000,500,10,0.5,8.919999748468399,241.66778755187988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(run_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
