{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the same code as before, please solve the following exercises\n",
    "    1. Change the number of observations to 100,000 and see what happens.\n",
    "    2. Play around with the learning rate. Values like 0.0001, 0.001, 0.1, 1 are all interesting to observe. \n",
    "    3. Change the loss function. An alternative loss for regressions is the Huber loss. \n",
    "    The Huber loss is more appropriate than the L2-norm when we have outliers, as it is less sensitive to them (in our example we don't have outliers, but you will surely stumble upon a dataset with outliers in the future). The L2-norm loss puts all differences *to the square*, so outliers have a lot of influence on the outcome. \n",
    "    The proper syntax of the Huber loss is 'huber_loss'\n",
    "    \n",
    "    \n",
    "Useful tip: When you change something, don't forget to RERUN all cells. This can be done easily by clicking:\n",
    "Kernel -> Restart & Run All\n",
    "If you don't do that, your algorithm will keep the OLD values of all parameters.\n",
    "\n",
    "You can either use this file for all the exercises, or check the solutions of EACH ONE of them in the separate files we have provided. All other files are solutions of each problem. If you feel confident enough, you can simply change values in this file. Please note that it will be nice, if you return the file to starting position after you have solved a problem, so you can use the lecture as a basis for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must always import the relevant libraries for our problem at hand. NumPy and TensorFlow are required for this example.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We generate data using the exact same logic and code as the example from the previous notebook. The only difference now is that we save it to an npz file. Npz is numpy's file type which allows you to save numpy arrays into a single .npz file. We introduce this change because in machine learning most often: \n",
    "\n",
    "* you are given some data (csv, database, etc.)\n",
    "* you preprocess it into a desired format (later on we will see methods for preprocesing)\n",
    "* you save it into npz files (if you're working in Python) to access later\n",
    "\n",
    "Nothing to worry about - this is literally saving your NumPy arrays into a file that you can later access, nothing more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we should declare a variable containing the size of the training set we want to generate.\n",
    "observations = 100000\n",
    "\n",
    "# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.\n",
    "# We have picked x and z, since it is easier to differentiate them.\n",
    "# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).\n",
    "# The size of xs and zs is observations x 1. In this case: 1000 x 1.\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "# Combine the two dimensions of the input into one input matrix. \n",
    "# This is the X matrix from the linear model y = x*w + b.\n",
    "# column_stack is a Numpy method, which combines two matrices (vectors) into one.\n",
    "generated_inputs = np.column_stack((xs,zs))\n",
    "\n",
    "# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.\n",
    "# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.\n",
    "generated_targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# save into an npz file called \"TF_intro\"\n",
    "np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with TensorFlow\n",
    "\n",
    "<i/>Note: This intro is just the basics of TensorFlow which has way more capabilities and depth than that.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from the NPZ\n",
    "training_data = np.load('TF_intro.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples\n",
      "Epoch 1/100\n",
      "100000/100000 - 2s - loss: 0.6640\n",
      "Epoch 2/100\n",
      "100000/100000 - 2s - loss: 0.3788\n",
      "Epoch 3/100\n",
      "100000/100000 - 2s - loss: 0.3845\n",
      "Epoch 4/100\n",
      "100000/100000 - 2s - loss: 0.3812\n",
      "Epoch 5/100\n",
      "100000/100000 - 2s - loss: 0.3788\n",
      "Epoch 6/100\n",
      "100000/100000 - 2s - loss: 0.3824\n",
      "Epoch 7/100\n",
      "100000/100000 - 2s - loss: 0.3755\n",
      "Epoch 8/100\n",
      "100000/100000 - 2s - loss: 0.3779\n",
      "Epoch 9/100\n",
      "100000/100000 - 2s - loss: 0.3795\n",
      "Epoch 10/100\n",
      "100000/100000 - 2s - loss: 0.3771\n",
      "Epoch 11/100\n",
      "100000/100000 - 2s - loss: 0.3792\n",
      "Epoch 12/100\n",
      "100000/100000 - 2s - loss: 0.3780\n",
      "Epoch 13/100\n",
      "100000/100000 - 2s - loss: 0.3803\n",
      "Epoch 14/100\n",
      "100000/100000 - 2s - loss: 0.3796\n",
      "Epoch 15/100\n",
      "100000/100000 - 2s - loss: 0.3821\n",
      "Epoch 16/100\n",
      "100000/100000 - 2s - loss: 0.3794\n",
      "Epoch 17/100\n",
      "100000/100000 - 2s - loss: 0.3820\n",
      "Epoch 18/100\n",
      "100000/100000 - 2s - loss: 0.3786\n",
      "Epoch 19/100\n",
      "100000/100000 - 2s - loss: 0.3785\n",
      "Epoch 20/100\n",
      "100000/100000 - 2s - loss: 0.3814\n",
      "Epoch 21/100\n",
      "100000/100000 - 2s - loss: 0.3795\n",
      "Epoch 22/100\n",
      "100000/100000 - 2s - loss: 0.3812\n",
      "Epoch 23/100\n",
      "100000/100000 - 2s - loss: 0.3791\n",
      "Epoch 24/100\n",
      "100000/100000 - 2s - loss: 0.3818\n",
      "Epoch 25/100\n",
      "100000/100000 - 2s - loss: 0.3809\n",
      "Epoch 26/100\n",
      "100000/100000 - 2s - loss: 0.3790\n",
      "Epoch 27/100\n",
      "100000/100000 - 2s - loss: 0.3812\n",
      "Epoch 28/100\n",
      "100000/100000 - 2s - loss: 0.3764\n",
      "Epoch 29/100\n",
      "100000/100000 - 2s - loss: 0.3789\n",
      "Epoch 30/100\n",
      "100000/100000 - 2s - loss: 0.3822\n",
      "Epoch 31/100\n",
      "100000/100000 - 2s - loss: 0.3799\n",
      "Epoch 32/100\n",
      "100000/100000 - 2s - loss: 0.3831\n",
      "Epoch 33/100\n",
      "100000/100000 - 2s - loss: 0.3799\n",
      "Epoch 34/100\n",
      "100000/100000 - 2s - loss: 0.3820\n",
      "Epoch 35/100\n",
      "100000/100000 - 2s - loss: 0.3776\n",
      "Epoch 36/100\n",
      "100000/100000 - 2s - loss: 0.3817\n",
      "Epoch 37/100\n",
      "100000/100000 - 2s - loss: 0.3797\n",
      "Epoch 38/100\n",
      "100000/100000 - 2s - loss: 0.3814\n",
      "Epoch 39/100\n",
      "100000/100000 - 2s - loss: 0.3791\n",
      "Epoch 40/100\n",
      "100000/100000 - 2s - loss: 0.3827\n",
      "Epoch 41/100\n",
      "100000/100000 - 2s - loss: 0.3837\n",
      "Epoch 42/100\n",
      "100000/100000 - 2s - loss: 0.3779\n",
      "Epoch 43/100\n",
      "100000/100000 - 2s - loss: 0.3826\n",
      "Epoch 44/100\n",
      "100000/100000 - 2s - loss: 0.3800\n",
      "Epoch 45/100\n",
      "100000/100000 - 3s - loss: 0.3785\n",
      "Epoch 46/100\n",
      "100000/100000 - 2s - loss: 0.3829\n",
      "Epoch 47/100\n",
      "100000/100000 - 2s - loss: 0.3775\n",
      "Epoch 48/100\n",
      "100000/100000 - 2s - loss: 0.3801\n",
      "Epoch 49/100\n",
      "100000/100000 - 2s - loss: 0.3826\n",
      "Epoch 50/100\n",
      "100000/100000 - 2s - loss: 0.3833\n",
      "Epoch 51/100\n",
      "100000/100000 - 2s - loss: 0.3807\n",
      "Epoch 52/100\n",
      "100000/100000 - 2s - loss: 0.3797\n",
      "Epoch 53/100\n",
      "100000/100000 - 2s - loss: 0.3832\n",
      "Epoch 54/100\n",
      "100000/100000 - 2s - loss: 0.3807\n",
      "Epoch 55/100\n",
      "100000/100000 - 2s - loss: 0.3821\n",
      "Epoch 56/100\n",
      "100000/100000 - 2s - loss: 0.3811\n",
      "Epoch 57/100\n",
      "100000/100000 - 2s - loss: 0.3808\n",
      "Epoch 58/100\n",
      "100000/100000 - 2s - loss: 0.3808\n",
      "Epoch 59/100\n",
      "100000/100000 - 2s - loss: 0.3787\n",
      "Epoch 60/100\n",
      "100000/100000 - 2s - loss: 0.3820\n",
      "Epoch 61/100\n",
      "100000/100000 - 2s - loss: 0.3776\n",
      "Epoch 62/100\n",
      "100000/100000 - 2s - loss: 0.3811\n",
      "Epoch 63/100\n",
      "100000/100000 - 2s - loss: 0.3819\n",
      "Epoch 64/100\n",
      "100000/100000 - 2s - loss: 0.3794\n",
      "Epoch 65/100\n",
      "100000/100000 - 2s - loss: 0.3828\n",
      "Epoch 66/100\n",
      "100000/100000 - 2s - loss: 0.3818\n",
      "Epoch 67/100\n",
      "100000/100000 - 2s - loss: 0.3838\n",
      "Epoch 68/100\n",
      "100000/100000 - 2s - loss: 0.3767\n",
      "Epoch 69/100\n",
      "100000/100000 - 2s - loss: 0.3826\n",
      "Epoch 70/100\n",
      "100000/100000 - 2s - loss: 0.3819\n",
      "Epoch 71/100\n",
      "100000/100000 - 2s - loss: 0.3821\n",
      "Epoch 72/100\n",
      "100000/100000 - 2s - loss: 0.3823\n",
      "Epoch 73/100\n",
      "100000/100000 - 2s - loss: 0.3789\n",
      "Epoch 74/100\n",
      "100000/100000 - 2s - loss: 0.3822\n",
      "Epoch 75/100\n",
      "100000/100000 - 2s - loss: 0.3796\n",
      "Epoch 76/100\n",
      "100000/100000 - 2s - loss: 0.3763\n",
      "Epoch 77/100\n",
      "100000/100000 - 2s - loss: 0.3821\n",
      "Epoch 78/100\n",
      "100000/100000 - 2s - loss: 0.3842\n",
      "Epoch 79/100\n",
      "100000/100000 - 2s - loss: 0.3814\n",
      "Epoch 80/100\n",
      "100000/100000 - 2s - loss: 0.3840\n",
      "Epoch 81/100\n",
      "100000/100000 - 2s - loss: 0.3828\n",
      "Epoch 82/100\n",
      "100000/100000 - 2s - loss: 0.3834\n",
      "Epoch 83/100\n",
      "100000/100000 - 2s - loss: 0.3812\n",
      "Epoch 84/100\n",
      "100000/100000 - 2s - loss: 0.3774\n",
      "Epoch 85/100\n",
      "100000/100000 - 2s - loss: 0.3816\n",
      "Epoch 86/100\n",
      "100000/100000 - 2s - loss: 0.3840\n",
      "Epoch 87/100\n",
      "100000/100000 - 2s - loss: 0.3809\n",
      "Epoch 88/100\n",
      "100000/100000 - 2s - loss: 0.3803\n",
      "Epoch 89/100\n",
      "100000/100000 - 2s - loss: 0.3815\n",
      "Epoch 90/100\n",
      "100000/100000 - 2s - loss: 0.3823\n",
      "Epoch 91/100\n",
      "100000/100000 - 2s - loss: 0.3806\n",
      "Epoch 92/100\n",
      "100000/100000 - 2s - loss: 0.3801\n",
      "Epoch 93/100\n",
      "100000/100000 - 2s - loss: 0.3802\n",
      "Epoch 94/100\n",
      "100000/100000 - 2s - loss: 0.3810\n",
      "Epoch 95/100\n",
      "100000/100000 - 2s - loss: 0.3798\n",
      "Epoch 96/100\n",
      "100000/100000 - 2s - loss: 0.3808\n",
      "Epoch 97/100\n",
      "100000/100000 - 2s - loss: 0.3832\n",
      "Epoch 98/100\n",
      "100000/100000 - 2s - loss: 0.3810\n",
      "Epoch 99/100\n",
      "100000/100000 - 2s - loss: 0.3801\n",
      "Epoch 100/100\n",
      "100000/100000 - 2s - loss: 0.3803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14bc80510>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare a variable where we will store the input size of our model\n",
    "# It should be equal to the number of variables you have\n",
    "input_size = 2\n",
    "# Declare the output size of the model\n",
    "# It should be equal to the number of outputs you've got (for regressions that's usually 1)\n",
    "output_size = 1\n",
    "\n",
    "# Outline the model\n",
    "# We lay out the model in 'Sequential'\n",
    "# Note that there are no calculations involved - we are just describing our network\n",
    "model = tf.keras.Sequential([\n",
    "                            # Each 'layer' is listed here\n",
    "                            # The method 'Dense' indicates, our mathematical operation to be (xw + b)\n",
    "                            tf.keras.layers.Dense(output_size,\n",
    "                                                 # there are extra arguments you can include to customize your model\n",
    "                                                 # in our case we are just trying to create a solution that is \n",
    "                                                 # as close as possible to our NumPy model\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])\n",
    "\n",
    "# We can also define a custom optimizer, where we can specify the learning rate\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "# Note that sometimes you may also need a custom loss function \n",
    "# That's much harder to implement and won't be covered in this course though\n",
    "\n",
    "# 'compile' is the place where you select and indicate the optimizers and the loss\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# finally we fit the model, indicating the inputs and targets\n",
    "# if they are not otherwise specified the number of epochs will be 1 (a single epoch of training), \n",
    "# so the number of epochs is 'kind of' mandatory, too\n",
    "# we can play around with verbose; we prefer verbose=2\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the weights and bias\n",
    "Extracting the weight(s) and bias(es) of a model is not an essential step for the machine learning process. In fact, usually they would not tell us much in a deep learning context. However, this simple example was set up in a way, which allows us to verify if the answers we get are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.9958203],\n",
       "        [-3.0167472]], dtype=float32), array([5.0077157], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the weights and biases is achieved quite easily\n",
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.9952769],\n",
       "       [-2.994508 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can save the weights and biases in separate variables for easier examination\n",
    "# Note that there can be hundreds or thousands of them!\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.995036], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can save the weights and biases in separate variables for easier examination\n",
    "# Note that there can be hundreds or thousands of them!\n",
    "bias = model.layers[0].get_weights()[1]\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the outputs (make predictions)\n",
    "Once more, this is not an essential step, however, we usually want to be able to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-620045f65402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Sometimes it is useful to round the values to be able to read the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Usually we use this method on NEW DATA, rather than our original training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "# We can predict new values in order to actually make use of the model\n",
    "# Sometimes it is useful to round the values to be able to read the output\n",
    "# Usually we use this method on NEW DATA, rather than our original training data\n",
    "model.predict_on_batch(training_data['inputs']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we display our targets (actual observed values), we can manually compare the outputs and the targets\n",
    "training_data['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is optimized, so the outputs are calculated based on the last form of the model\n",
    "\n",
    "# We have to np.squeeze the arrays in order to fit them to what the plot function expects.\n",
    "# Doesn't change anything as we cut dimensions of size 1 - just a technicality.\n",
    "plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()\n",
    "\n",
    "# Voila - what you see should be exactly the same as in the previous notebook!\n",
    "# You probably don't see the point of TensorFlow now - it took us the same number of lines of code\n",
    "# to achieve this simple result. However, once we go deeper in the next chapter,\n",
    "# TensorFlow will save us hundreds of lines of code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
